{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and CBOW (Continuous Bag of Words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Ambiguity**\n",
    "- We want same things to share the same weight.\n",
    "- We need to learn how words are related to do this.\n",
    "    - And to do this, we need a lot of labelled data.\n",
    "- Trend\n",
    "    - Similar words tend to occur in similar contexts.\n",
    "    - ![](cbow1.png)\n",
    "        - If the model can predict a word's context would treat \"cat\" and \"kitty\" similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**\n",
    "- We are going to map words to small vectors (embeddings).\n",
    "    - Close to each other for words with same meanings.\n",
    "    - Far apart when they have different meanings.\n",
    "    - Embedding solves some of the sparsity problem.\n",
    "    - We have a word representation where all the cat-like things are represented by vectors that are similar.\n",
    "    - It can generalize from this pattern of cat-like things.\n",
    "    - ![](cbow2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**\n",
    "- Simple model to embed words.\n",
    "- For each word in the sentence, we'll map it to an embedding.\n",
    "    - We'll use the embedding to predict the context of the word.\n",
    "    - The context: word nearby (in the window).\n",
    "    - We'll use a simple logistic regression to predict the context (word nearby) as like any supervised learning problem where our context is our target.\n",
    "    - ![](cbow3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE**\n",
    "- One way we see embeddings are clustering is to use a nearest neighbour look-up.\n",
    "    - ![](cbow4.png)\n",
    "- Another way is to reduce dimensionality to 2d.\n",
    "    - Normally we would use PCA. But this would lose critical information.\n",
    "        - ![](cbow5.png)\n",
    "    - We can preserve the neighbourhood structure using t-SNE.\n",
    "        - ![](cbow6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "- [Visualizing Data using t-SNE](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Important Notes**\n",
    "1. Comparing Embeddings.\n",
    "    - Normally we would use the L2 way of calculating the distance between 2 vectors.\n",
    "    - But it would be better to measure the closeness to use cosine because the length is not relevant.\n",
    "2. Instead of comparing our softmax results with our labelled data, we can sample the words that are not the targets.\n",
    "    - Pick only a handful of them and act other words are not there.\n",
    "    - This is called softmax where we make things faster with no performance issues.\n",
    "        - ![](cbow7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Analogies**\n",
    "- ![](cbow8.png)\n",
    "- ![](cbow9.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

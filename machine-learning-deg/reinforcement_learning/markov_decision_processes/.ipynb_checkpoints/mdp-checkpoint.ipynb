{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences in Learning**\n",
    "- Supervised learning: y = f(x)\n",
    "    - Given x-y pairs\n",
    "    - Learn function f(x)\n",
    "        - Function approximation\n",
    "- Unsupervised learning: f(x)\n",
    "    - Given x's\n",
    "    - Learn the function f(x)\n",
    "        - Clustering description\n",
    "- Reinforcement learning: \n",
    "    - Given x's and z's\n",
    "    - Learn function f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Decision Processes**\n",
    "- **States: S**\n",
    "    - They are kind of like positions on a map if you are navigating to an end point\n",
    "- **Model (Transition Function): T(s, a, s') ~ P(s' | s, a)**\n",
    "    - The model is like a set of rules for a game (physics of the world).\n",
    "    - s: state\n",
    "    - a: action\n",
    "    - s': another state\n",
    "    - Probability of s' given s and a\n",
    "        - Simple: if you're in a deterministic world, if you go straight (a) from the toilet (s), you would go straight (s') with a probability of 1. \n",
    "        - Slightly complex: can be that you have a probability of only 0.8 if you choose to go up, you may go right with a probability of 0.2. \n",
    "    - Your potential state (s') is only dependent on your current state (s) if not you will have more s-es.\n",
    "        - This is a Markovian property where only the present matters.\n",
    "        - Also, given the present state, the future and the past are independent.\n",
    "- **Actions: A(s), A**\n",
    "    - They are things you can do such as going up, down, left and right if you are navigating through a box grid. \n",
    "    - They can be other kinds of actions too, depending on your scenario. \n",
    "- **Reward: R(s), R(s, a), R(s, a, s')**\n",
    "    - Scalar value for being in a state.\n",
    "        - Position 1, you'll have 100 points.\n",
    "        - Position 2, you'll have -10 points.\n",
    "    - R(s): reward for being in a state\n",
    "    - R(s, a): reward for being in a state and taking an action\n",
    "    - R(s, a, s'): reward for being in a state, taking an action and ending up in the other state\n",
    "        - They are mathematically equivalent.\n",
    "- **Policy: π(s) → a**\n",
    "    - This is basically the solution: what action to take in any particular state.\n",
    "    - It takes in a state, s, and it tells you the action, a, to take.\n",
    "    - π* optimizes the amount the reward at any given point in time.\n",
    "        - Here we have (s, a, r) triplets. \n",
    "            - We need to find the optimal action\n",
    "            - Given what we know above\n",
    "                - π*: function f(x)\n",
    "                - r: z \n",
    "                - s: x \n",
    "                - a: y\n",
    "- **Visualization**\n",
    "    - ![](mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rewards (Domain Knowledge)**\n",
    "- **Delayed rewards**\n",
    "    - You won't know what your immediate action would lead to down the road.\n",
    "        - Example in chess\n",
    "            - You did many moves\n",
    "            - Then you have your final reward where you did well or badly\n",
    "                - You could have done well at the start then worst at the end.\n",
    "                - You could have done badly at the start then better at the end.\n",
    "- **Example**\n",
    "    - R(s) = -0.01\n",
    "        - Reward for every state except for the 2 absorbing states is -0.01.\n",
    "        - s_good = +1\n",
    "        - s_bad = -0.01\n",
    "        - Each action you take, you get punished.\n",
    "            - You are encouraged to end the game soon.\n",
    "        - Minor changes to your reward function matters a lot.\n",
    "            - If you have a huge negative reward, you will be in a hurry.\n",
    "            - If you have a small negative reward, you may not be in a hurry to reach the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence of rewards: assumptions**\n",
    "- **Stationary preferences**\n",
    "    - [a1, a2, ...] > [b1, b2, ...]\n",
    "    - [r, a1, a2, ...] > [r, b1, b2, ...]\n",
    "    - You'll do the same action regardless of time.\n",
    "- **Inifinite horizons** \n",
    "    - The game does not end until we reach the absorbing state. \n",
    "    - If we do not have an inifinite time, and a short amount of time, we may want to rush through to get a positive reward.\n",
    "        - **Policy with finite horizon: π(s, t) → a**\n",
    "            - This would terminate after t\n",
    "            - Gives non-stationary polices where states' utilities changes with time.\n",
    "- **Utility of sequences**\n",
    "    - We will be adding rewards using discounted sums.\n",
    "    - $$ U(S_0 S_1 ...) = \\sum_{i=0}^∞ γ^tR(S_t) = \\frac {R_{max}}{1 - γ}$$\n",
    "        - $$ 0 ≤ γ < 1 $$\n",
    "        - If we do not multiply by gamma, all the rewards would sum to infinity.\n",
    "        - When we multiply by gamma, we get a geometric series that allows us to multiply an infinite number of rewards to get a finite number.\n",
    "        - Also, the discount factor is multiplied because sooner rewards probably have higher utility than later rewards.\n",
    "            - Things in the future are discounted so much they become negligible. Hence, we reach a finite number for our utility.\n",
    "- **Absorbing state**\n",
    "    - Guarantee that for every policy, a terminal state will eventually be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Policy**\n",
    "- $$ π^* = argmax_π \\ E \\ [ \\sum_{i=0}^∞γ^tR(s_t) \\ | \\ π ] $$\n",
    "    - Expectation because it is non-deterministic\n",
    "    - π*: optimal policy\n",
    "- $$ U^π(s) = E \\ [ \\sum_{i=0}^∞γ^tR(s_t) \\ | \\ π, \\ s_0 = s ] $$\n",
    "    - Utility of a particular state depends on the policy we are taking π\n",
    "    - Utility is what we expect to see from then. \n",
    "    - This is not equal to the reward, R(s) for being in that state that is immediate.\n",
    "        - If you look at going to university, there's an immediate negative reward, say paying for school fees. But in the long-run you may have higher salary which is similar to U(s), a form of delayed reward.\n",
    "    - Utility, U(s) gives us a long-term view.\n",
    "- $$ π^*(s) = argmax_a \\sum_{s'}T(s, a, s') U(s') $$\n",
    "    - This is the optimal action to take at a state. \n",
    "    - For every state, returns an action that maximizes my expected utility. \n",
    "- $$ U(s) = R(s) + γ \\ argmax_a \\sum_{s'}T(s, a, s') U(s')  $$\n",
    "    - True utility of that state U(s): R(S) + expected utility\n",
    "    - Belman equation.\n",
    "        - Reward of that state plus the discount of all the reward from then on.\n",
    "            - T(s, a, s'): model.\n",
    "            - U(s'): utility of another state.\n",
    "    - n equations: since we've n number of states.\n",
    "    - n unknowns: U(s') since we've n number of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Policies: Value Iteration**\n",
    "1. We start with arbitrary utilities\n",
    "    - For example, 0.\n",
    "2. Update utilities based on neighbours (all the states it can reach). \n",
    "    - $$ \\hat {U}_{t+1}(s) = R(s) + γ \\ argmax_a \\sum_{s'}T(s, a, s') \\ \\hat {U}_t(s')  $$\n",
    "    - Bellman update equation.\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Policies: Policy Iteration**\n",
    "1. Start with by guessing\n",
    "    -  $$π_0$$\n",
    "2. Evaluate\n",
    "    - Given\n",
    "        - $$π_t$$\n",
    "    - Calculate\n",
    "        - $$u_t = u^{π_t}$$\n",
    "3. Improve: maximizing expected utility\n",
    "    - $$ π_{t+1} = argmax_a \\sum_{s'}T(s, a, s') \\ U_t(s') $$\n",
    "    - $$ U_t(s) = R(s) + γ \\ \\sum_{s'}T(s, a, s') U_t(s')  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "- https://www.youtube.com/watch?v=W1S-HSakPTM&list=PL6MuV0DF6AuoviA41dtji6q-PM4hvAcNk&index=14\n",
    "- http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

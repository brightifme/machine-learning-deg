{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks Classification Model\n",
    "- Intuition of a perceptron \n",
    "    - Linear function that computes hyperplanes\n",
    "    - You have the following inputs and weights\n",
    "        - X, inputs\n",
    "        - W, weights\n",
    "        - Activation = sum(X*W) >= θ\n",
    "            - θ is the threshold\n",
    "            - If Activation >= θ\n",
    "                - Yes, y=1\n",
    "            - If Activation <= θ \n",
    "                - No, y=0\n",
    "\n",
    "_I have a more [technical guide](http://www.ritchieng.com/neural-networks-learning/) by Andrew Ng that will give you clarity on the exact mathematics going behind a neural network._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Perceptron rule (threshold)\n",
    "2. Gradient descent or delta rule (unthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron rule**\n",
    "- Finite time for linearly separable data\n",
    "- ![](nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We include θ as a weight to make the math easier such that \n",
    "    - y_hat = sum(X*W) >= 0\n",
    "    - W includes the bias 1\n",
    "- The algorithm (3 lines) will run continuously\n",
    "- This only works on data that is linearly separable\n",
    "- Intuition of algorithm\n",
    "    - When y = y_hat = 0 or y = y_hat = 1\n",
    "        - This means we've **predicted correctly**\n",
    "        - So there's no need for learning, Δwi = 0\n",
    "    - When y = 0, y_hat = 1 or y = 1, y_hat = 0\n",
    "        - This means we've **predicted wrongly**\n",
    "        - There is a need for learning, Δwi would be non-zero\n",
    "        - We want to make sure we don't make huge changes, so that's where the learning rate comes in\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "- More robust for non-linear separability\n",
    "- ![](nn2.png)\n",
    "    - Here, we do not have a threshold\n",
    "    - We simply minimize the error directly\n",
    "    - We will add a learning rate to the derivative which is essentially Δwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of learning rules**\n",
    "- ![](nn3.png)\n",
    "    - Perceptron has a guarantee of finite convergence due to linear separability\n",
    "    - Gradient descent uses calculus but it converges to the limit\n",
    "        - We can't take the derivative of y_hat because y_hat holds binary values, 0 or 1\n",
    "        - However, this can be solved with a sigmoid function which we will be explaining subsequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a Network**\n",
    "- We can build a network for any boolean function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid: Differentiable Threshold**\n",
    "- ![](nn4.png)\n",
    "- If you take the derivative of the sigmoid function, it looks great! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "- Many local optima\n",
    "- Neural network with 2 hidden layers\n",
    "    - ![](https://upload.wikimedia.org/wikipedia/commons/7/7f/Two_layer_ann.svg)\n",
    "    - Each circle is a sigmoid function\n",
    "- It uses backpropagation\n",
    "    - Computationally beneficial organization of the chain rule\n",
    "    - Calculates errors of the actual value versus the predicted value and send it backwards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing (learning) weights**\n",
    "1. Gradient Descent\n",
    "2. Advanced methods\n",
    "    - Momentary\n",
    "    - Higher order derivatives\n",
    "    - Randomized optimization\n",
    "    - Penalty for \"complexity\"\n",
    "        - For neural networks, it gets complicated with more nodes and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restriction Bias**\n",
    "- Representational power\n",
    "    - What it is you're able to represent\n",
    "- Set of hypotheses that we will consider\n",
    "- Perceptron: \n",
    "    - Linear\n",
    "    - Half spaces\n",
    "- Sigmoids: \n",
    "    - Much more complex\n",
    "    - Not much restriction\n",
    "- What kind of functions can we represent?\n",
    "    - Boolean: \n",
    "        - Network of threshold-like units\n",
    "    - Continuous function:\n",
    "        - As input changes, output changes \n",
    "        - Connected, no jumps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preference Bias**\n",
    "- Algorithm's selection of one representation over another\n",
    "- What algorithm?\n",
    "    - We have to initialize the weights with some values\n",
    "    - We can start with small random values\n",
    "- Occan's razor\n",
    "    - Entities should not be multiplied unnecessarily\n",
    "        - Necessary when we're better fitting the data\n",
    "        - Choose the one that is less complex\n",
    "        - We'll get better generalization with simpler hypotheses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually create perceptron's algorithm**\n",
    "- activate() function \n",
    "    - y_hat = sum(X*W) >= θ\n",
    "        - sum(X*W) > θ\n",
    "            - 1\n",
    "        - sum(X*W) < θ\n",
    "            - 0\n",
    "- update() function\n",
    "    - w_i = w_i + Δw_i\n",
    "    - Δw_i = (η)(y - y_hat)(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assume this file's name is p1_activation.py\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    # Special method that is automatically called when an object of that Class is created\n",
    "    # p1 = Perceptron() is called an instantiation\n",
    "    # Here, we gave default values of weights and threshold in case no arguments are passed during instantiation\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    # This is the method\n",
    "    # We can access it after instantiation and by passing an argument, inputs\n",
    "    # p1.activate(argument)\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        # For each data point...\n",
    "            # 1. Obtain the neuron's prediction for that point, y_hat\n",
    "            # 2. Update self.weights based on prediction accuracy, learning rate and input value\n",
    "                # Δw_i = (η)(y - y_hat)(x_i)\n",
    "                # w_i = w_i + Δw_i \n",
    "        \n",
    "        # Explanation of enumerate function below\n",
    "        for (index, row) in enumerate(values):\n",
    "            self.weights = self.weights + eta*(train[index] - self.activate(row))*row\n",
    "        \n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    # assert tests if the statement is true or false\n",
    "    \n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    # Instantiation of class Perceptron\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "# _name_ is a global variable\n",
    "# if _name_ == \"_main_\" would be true if you run a.py\n",
    "# if _name_ == \"_main_\" would not be true if you run b.py containing an import statement importing p1_activation.py\n",
    "    # _name_ would be \"p1_activation\", the module we are \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pizza\n",
      "1 pasta\n",
      "2 salad\n",
      "3 nachos\n"
     ]
    }
   ],
   "source": [
    "# Enumerate function example\n",
    "choices = ['pizza', 'pasta', 'salad', 'nachos']\n",
    "for index, item in enumerate(choices):\n",
    "    print(index, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layered Network Calculation**\n",
    "- ![](https://cdn-enterprise.discourse.org/udacity/uploads/default/original/3X/9/4/942a1b4ed344d005e2ba810380f049e8cfdf23fb.png)\n",
    "- What will be the network output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight scalar 1 (1, 3)\n",
      "weight scalar 2 (1, 3)\n",
      "output sclar (2,)\n",
      "input scalar (3,)\n",
      "overall weights matrix (2, 3)\n",
      "Nodes matrix (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-25])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layered network calculation\n",
    "\n",
    "# Hidden layer weights\n",
    "w_h1_1 = np.array([1, 1, -5]).reshape(1, 3)\n",
    "w_h1_2 = np.array([3, -4, 2]).reshape(1, 3)\n",
    "\n",
    "# We can concatenate the matrices to form 2 x 3\n",
    "weights = np.concatenate((w_h1_1, w_h1_2), axis=0)\n",
    "\n",
    "# Output weights\n",
    "w_o = np.array([2, -1])\n",
    "\n",
    "# Input \n",
    "inp = np.array([1, 2, 3])\n",
    "\n",
    "# Shapes to understand which matrix to reshape\n",
    "print('weight scalar 1', w_h1_1.shape)\n",
    "print('weight scalar 2', w_h1_2.shape)\n",
    "print('output sclar', w_o.shape)\n",
    "print('input scalar', inp.shape)\n",
    "print('overall weights matrix', weights.shape)\n",
    "\n",
    "# matrix of ndoes\n",
    "# (2, 3) . (3 x 1) would give (2 x 1)\n",
    "nodes_matrix = np.dot(weights, inp).reshape(1, 2)\n",
    "print('Nodes matrix', nodes_matrix.shape)\n",
    "\n",
    "# (1 x 2), nodes\n",
    "# multiplied by (2 x 1), output\n",
    "# would give (1 x 1)\n",
    "np.dot(nodes_matrix, w_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Rule: Intuitive Explanation**\n",
    "- (Prior probability)(Test evidence) --> (Posterior probability)\n",
    "- Example\n",
    "    - P(C) = 0.01\n",
    "    - 90% it is positive if you have C (Sensitivity)\n",
    "    - 90% it is negative if you don't have C (Specificity)\n",
    "        - **prior**\n",
    "            - P(C) = 0.01\n",
    "            - P(C') = 0.99 \n",
    "            - P(Pos|C) = 0.9 \n",
    "                - P(Pos|C') = 0.1\n",
    "            - P(Neg|C') = 0.9\n",
    "                - P(Neg|C) = 0.1\n",
    "        - **joint**\n",
    "            - P(C and Pos) = P(C)P(Pos|C) = (0.01)(0.9) = 0.009\n",
    "            - P(C'and Pos) = P(C')P(Pos|C') = (0.99)(0.1) = 0.099\n",
    "        - **normalizer**\n",
    "            - P(Pos) = P(C and Pos) + P(C' and Pos) = 0.108\n",
    "        - **posterior**\n",
    "            - P(C|Pos) = 0.009 / 0.108 = 0.0833\n",
    "            - P(C'|Pos) = 0.099 / 0.108 = 0.9167\n",
    "                - Adding both = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Rule: Example**\n",
    "- This is really good for text learning\n",
    "- Example\n",
    "    - P(Chris) = 0.5 \n",
    "        - P(Love|Chris) = 0.1\n",
    "        - P(Deal|Chris) = 0.8\n",
    "        - P(Life|Chris) = 0.1\n",
    "    - P(Sara) = 0.5\n",
    "        - P(Love|Sara) = 0.5\n",
    "        - P(Love|Deal) = 0.2\n",
    "        - P(Love|Life) = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444445\n",
      "0.5555555555555555\n"
     ]
    }
   ],
   "source": [
    "p_chris_and_love_deal = 0.1*0.8*0.5\n",
    "\n",
    "p_sara_and_love_deal = 0.5*0.2*0.5\n",
    "\n",
    "normalizer = p_chris_and_love_deal + p_sara_and_love_deal\n",
    "\n",
    "p_chris_given_love_deal = p_chris_and_love_deal / normalizer\n",
    "p_sara_given_love_deal = p_sara_and_love_deal / normalizer\n",
    "\n",
    "# P(Chris | \"Love Deal\")\n",
    "print(p_chris_given_love_deal)\n",
    "\n",
    "# P(Sara | \"Love Deal\")\n",
    "print(p_sara_given_love_deal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Rule: Theory**\n",
    "- Learn the best hypothesis given data and some domain knowledge\n",
    "    - Learn the most probable hypothesis given data and domain knowledge\n",
    "    - **$$\\underset{h∈H}{\\mathrm{argmax}}P(h|D)$$**\n",
    "        - h: some hypothesis\n",
    "        - D: some data\n",
    "        - argmax h∈H\n",
    "            - We want to maximize P(h|D)\n",
    "- Bayes rule\n",
    "    - **$$P(h|D) = \\frac{P(D|h)P(h)}{P(D)}$$**\n",
    "        - $$P(a,b) = P(a|b)P(b)$$\n",
    "        - $$P(a,b) = P(b|a)P(a)$$\n",
    "            - P(a,b) is the probability of a and b\n",
    "        - P(D)\n",
    "            - This is a normalizing term\n",
    "            - Prior on the data\n",
    "        - P(D|h)\n",
    "            - Data given the hypothesis\n",
    "            - $$D =\\{x_i, d_i\\}$$\n",
    "            - Training data, D, with inputs (x) and labels (d)\n",
    "            - What's the likelihood that given all of x_i and P(D|h) hypothesis is true, we will observe d's\n",
    "        - P(h)\n",
    "            - Prior on h \n",
    "            - Domain knowledge\n",
    "                - Say if you use KNN, you believe points close together would give similar outputs with higher likelihood than those far from one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Learning Algorithm**\n",
    "- For each h∈H, calculate P(h|D) ≈ P(D|h) \n",
    "    - Output\n",
    "        - $$h_1 = \\underset{h}{\\mathrm{argmax}}P(h|D)$$\n",
    "            - h_1: Maximum a posteriori\n",
    "        - $$h_2 = \\underset{h}{\\mathrm{argmax}}P(D|h)$$\n",
    "            - h_2: Maximum likelihood\n",
    "            - We're assuming P(h) and P(D) are uniform \n",
    "                - They're constants so we can ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**\n",
    "- Ultimately we've simplified, using Gaussian distribution, to minimizing the sum of squared errors!\n",
    "- Based on bayes rule we've ended up deriving sum of squared error\n",
    "- ![](bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Classification**\n",
    "- The algorithm changes slightly here\n",
    "    - We are maximizing the weighted vote instead of simply P(h|D)\n",
    "    - ![](bayes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version Space**\n",
    "- The version space is a subset of the hypothesis space, where the hypotehesis space is a space of all possible hypotheses\n",
    "- The version space are those hypotheses such that they correctly predict the training data you have (essentially a 100% model fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Networks, Bayesian Nets, Belief Networks or Graphical Models**\n",
    "- Representing and dealing with probabilities\n",
    "- Conditional independence\n",
    "    - X is conditionally independent of Y given Z if the probability distribution governing X is independent of the value of y given the value of Z\n",
    "        - P(X=x | Y=y, Z=z) = P(X=x | Z=z)\n",
    "        - More compactly\n",
    "            - P(X|Y,Z) = P(X|Z)\n",
    "- Order of graph must be topological\n",
    "    - Graph must be acylic\n",
    "        - No cycles\n",
    "- Sampling\n",
    "    - Two things distributions are for\n",
    "        - Probability of value\n",
    "        - Generate values\n",
    "    - Reasons for sampling\n",
    "        - Simulation of a complex process\n",
    "        - Approximate inference\n",
    "            - For machines\n",
    "            - We can't find the exact values because it may be hard and slower\n",
    "        - Visualization\n",
    "            - For humans to get a feel\n",
    "- Inferencing Rules\n",
    "    - Marginalization\n",
    "        - $$P(x) = \\underset{y}\\sum(x,y)$$\n",
    "    - Chain rule\n",
    "        - $$P(x,y) = P(y|x)P(x) = P(y|x)P(y)$$\n",
    "    - Bayes rule\n",
    "        - $$P(y|x) = \\frac {P(x|y)P(y)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "- Say you've label A and B (hidden)\n",
    "    - Label A\n",
    "        - Have multiple words with different probabilities\n",
    "        - Every word gives evidence if it's label A\n",
    "        - We mutiply all the probabilities with the prior to find the joint probability of A\n",
    "    - Label B\n",
    "        - Have multiple words with different probabilities\n",
    "        - Every word gives evidence if it's label B\n",
    "        - We multiply all the probabilities with the prior to find the joint probability of B\n",
    "    - Now you can find out the probability of it being A or B\n",
    "- Reason why it's called Naive\n",
    "    - It ignores word order! \n",
    "- Equation\n",
    "    - $$p(C_{k}\\vert x_{1},\\dots ,x_{n})={\\frac  {1}{Z}}p(C_{k})\\prod _{{i=1}}^{n}p(x_{i}\\vert C_{k})$$$${\\hat  {y}}={\\underset  {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax}}}\\ p(C_{k})\\displaystyle \\prod _{{i=1}}^{n}p(x_{i}\\vert C_{k}).$$\n",
    "    - Here y_k is either Label A or Label B in the example above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Benefits**\n",
    "- Inference is cheap\n",
    "    - Linear \n",
    "- Few parameters\n",
    "- Estimate parameters with labeled data\n",
    "- Connects inference and classification\n",
    "- Empirically successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Training**\n",
    "- In the training process of a Bayes calssification problem, the sample data does the following:\n",
    "    1. Estimate likelihood distributions of X for each value of Y\n",
    "    2. Estimate prior probability P(Y=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes in Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create features' DataFrame and response Series\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92105263157894735"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate: create object\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees Classification Model\n",
    "- Supervised Learning\n",
    "- Classification\n",
    "    - Taking an input X\n",
    "    - Mapping to a discrete label y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification learning terms**\n",
    "- Instances\n",
    "    - Input\n",
    "        - Pictures\n",
    "        - Credit score examples\n",
    "- Concept\n",
    "    - Function (mappings from objects to members of a set)\n",
    "        - Takes input and maps the input to an output\n",
    "- Target concept\n",
    "    - The answer we want\n",
    "    - The function we want\n",
    "- Hypothesis Class\n",
    "    - Set of all concepts you're willing to entertain\n",
    "    - All functions we care about (classification-only)\n",
    "- Sample\n",
    "    - Training set\n",
    "- Candidate\n",
    "    - Concept that you think might be the target concept\n",
    "    - Concept = Target Concept\n",
    "- Testing set\n",
    "    - Take candidate concept and test if it does a good job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees Introduction**\n",
    "- It's simply asking a series of questions\n",
    "- You'll have decision nodes\n",
    "    - Pick an attribute and ask a question (is sex male?)\n",
    "        - Values = edges (lines)\n",
    "            - Yes \n",
    "                - Ask a different question (sub-node)                       \n",
    "            - No\n",
    "                - Ask a different question (sub-node)\n",
    "- ![Decision Tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "    - As you can see here, you can continue to ask more questions with more nodes down the tree.\n",
    "        - This is an example tree from the Titanic Survivors \n",
    "- Intuition\n",
    "    - Features' headers = questions\n",
    "        - sex_male\n",
    "        - age_>_9.5\n",
    "        - sibsp_>_2.5\n",
    "    - Features = answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees: Learning**\n",
    "1. Pick best attribute\n",
    "2. Ask question\n",
    "3. Follow the answer path\n",
    "4. Go to (1) \n",
    "    - Repeat until you get an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Decision Trees: Expressiveness**\n",
    "- n-OR: ANY\n",
    "    - You need n number (linear) of nodes\n",
    "        - Easy!\n",
    "- n-XOR: ODD PARITY\n",
    "    - If the number of attributes that are true = odd, output of the function would be true\n",
    "        - Otherwise, false\n",
    "    - You need 2^N number (exponential) of nodes\n",
    "        - Hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3 Algorithm**\n",
    "- A: best attribute\n",
    "- Assign A as decision attribute for node\n",
    "- For each value of A, create a descendant of node\n",
    "- Sort training examples to leaves\n",
    "- If examples perfectly classified, stop\n",
    "    - Else, iterate over leaves\n",
    "    \n",
    "**ID3: Bias**\n",
    "- Restriction Bias\n",
    "    - Restricted to hypothesis\n",
    "    - Example: only those considered in a decision tree\n",
    "- Preference Bias\n",
    "    - What sort of hypothesis we prefer\n",
    "- Inductive bias\n",
    "    - ID3 would prefer tree that has good splits near the top\n",
    "        - Prefers correct over incorrect\n",
    "        - Prefers shorter trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing Best Attributes: Information Gain**\n",
    "- Goal is to minimize entropy for maximum information gain\n",
    "- Formula\n",
    "    - ![](http://gisessence.com/wp-content/uploads/2016/05/CTA-f4-1.jpg)\n",
    "    - ![](https://gisessence.com/wp-content/uploads/2016/05/CTA-f2.jpg)\n",
    "- If all samples went to left side of tree \n",
    "    - No entropy gain in using that attribute\n",
    "    - Entropy constant\n",
    "- If all samples went to each side of the tree with no clear split (all mixed up)\n",
    "    - No entropy gain\n",
    "    - Entropy constant\n",
    "- If samples split into each side of the tree distinctly\n",
    "    - Max entropy gain\n",
    "    - Entropy fell \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees: Other Considerations**\n",
    "- What happens if we have continuous attributes?\n",
    "    - Age, weight, distance, etc.\n",
    "    - Create attribute with a range\n",
    "        - 20 <= age < 30\n",
    "        - This would return True or False\n",
    "- You can repeat an attribute along a path for continuous attributes\n",
    "    - Ask 20 <= age < 30\n",
    "    - Ask age > 30 etc.\n",
    "    - But it does not make sense to repeat an attribute for discrete attributes! \n",
    "- When do we stop?\n",
    "    - No overfitting (when the tree gets too big)!\n",
    "        - Solution? Pruning\n",
    "            - Hold out a validation set\n",
    "            - To build a full tree\n",
    "            - Cut leaves then check against validation set   \n",
    "            - If error is low enough, stop expanding the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn for Decision Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Instantiate\n",
    "iris = load_iris()\n",
    "\n",
    "# Create training and feature\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same 3-step process\n",
    "\n",
    "# 1. Instantiate\n",
    "# default criterion=gini\n",
    "# you can swap to criterion=entropy \n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# 2. Fit\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict, there're 4 features in the iris dataset\n",
    "y_pred_class = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating with Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with overfitting: min_samples_split**\n",
    "- default min_samples_split=2\n",
    "    - This means if you've 1 sample left in a node, you cannot split further\n",
    "    - When it gets really deep in depth, it overfits your data\n",
    "- If you increase your min_samples_split value\n",
    "    - You would decrease the depth of your tree\n",
    "    - This is because you would run out of samples to split \n",
    "    - This would reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Instantiate with min_samples_split = 50\n",
    "dtc = DecisionTreeClassifier(min_samples_split=4, random_state=0)\n",
    "\n",
    "# 2. Fit\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict, there're 4 features in the iris dataset\n",
    "y_pred_class = dtc.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GridSearchCV for optimizing parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=42, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Define the parameter values that should be searched\n",
    "sample_split_range = list(range(1, 50))\n",
    "\n",
    "# Create a parameter grid: map the parameter names to the values that should be searched\n",
    "# Simply a python dictionary\n",
    "# Key: parameter name\n",
    "# Value: list of values that should be searched for that parameter\n",
    "# Single key-value pair for param_grid\n",
    "param_grid = dict(min_samples_split=sample_split_range)\n",
    "\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(dtc, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid with data\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973214285714\n",
      "{'min_samples_split': 4}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=0, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy**\n",
    "- Controls how a Decision Tree decides where to split the data\n",
    "- Definition\n",
    "    - Measure of impurity in a bunch of examples\n",
    "    - Opposite of purity\n",
    "- Concept\n",
    "    - Find variables and split \n",
    "    - Split such that the sets contain the least impurities\n",
    "    - Repeat the process until sets are pure\n",
    "- Values Entropy can hold values between 0 and 1.0\n",
    "    - All examples same class\n",
    "        - Entropy = 0 \n",
    "    - All examples are evenly split amongst classes\n",
    "        - Entropy = 1.0 \n",
    "- Formula\n",
    "    - ![](https://gisessence.com/wp-content/uploads/2016/05/CTA-f2.jpg)\n",
    "        - p(i) is the fraction of examples in class i \n",
    "        - sum over all classes\n",
    "- Example of Formula\n",
    "    - Assume 2 classes for the node\n",
    "        - slow (2 examples)\n",
    "        - fast (2 examples)\n",
    "    - p(slow) = fraction of slow over all examples = 2/4 = 0.5\n",
    "    - p(fast) = fraction of fast over all examples = 2/4 = 0.5\n",
    "    - entropy = -p(slow)*log2(pslow) + (-p(fast)*log2(pslow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating impurity from the example above\n",
    "# Maximum state of impurity, 1.0\n",
    "import numpy as np\n",
    "-0.5*np.log2(0.5) + (-0.5*np.log2(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information gain**\n",
    "- Formula\n",
    "    - information gain = entropy(parent) - [weighted average]entropy(children)\n",
    "        - weight = number of examples in set / total number of examples\n",
    "- Decision tree algorithm: maximizing information gain\n",
    "- Example\n",
    "    - Assume the feature \"grade\" has the following details\n",
    "        - Steep (Slow, S)\n",
    "        - Steep (Slow, S)\n",
    "        - Flat (Fast, F)\n",
    "        - Steep (Fast, F)\n",
    "    - If we split based on \"grade\": is it steep?\n",
    "        - Child 1: Flat (Fast)\n",
    "        - Child 2: Steep (Slow, Slow, Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91829583405448956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy of child 1 = 0\n",
    "# Perfect split for this child\n",
    "\n",
    "# Entropy of child 2 = 0.918\n",
    "-(2/3)*np.log2(2/3) - (1/3)*np.log2(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6888"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighted average of entropy(children)\n",
    "(3/4)*(0.9184) + (1/4)*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31120000000000003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy Gain\n",
    "1 - (3/4)*(0.9184) + (1/4)*0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**\n",
    "- Prone to overfitting \n",
    "    - If you have a lot of features\n",
    "    - Stop growth of tree at the appropriate time\n",
    "- You can build bigger classifiers out of this\n",
    "    - It is called ensemble classifiers"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

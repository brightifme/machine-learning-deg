{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "- Support vector machines are models that learn to differentiate between data in two categories based on past examples\n",
    "- We want to have the maximum margin from the line to the points as shown in the diagram and that is the essence of SVMs\n",
    "    - The points close to the decision boundary matters, the rest are not important\n",
    "    - Note that SVMs aim to classify correctly before maximizing the margin\n",
    "    - ![](http://docs.opencv.org/2.4/_images/optimal-hyperplane.png)\n",
    "- You can also project your data into a higher dimensionality and split them with a hyperplane\n",
    "- Optimization problem for finding maximum margins uses quadratic programming\n",
    "- We use a kernel to identify similarity amongst points\n",
    "    - All kernels must satisfy Mercer conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM using Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create object\n",
    "iris = load_iris()\n",
    "\n",
    "# Create data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same 3 steps\n",
    "\n",
    "# 1. Instantiate\n",
    "# Default kernel='rbf'\n",
    "# We can change to others\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# 2. Fit\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict \n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy calculation\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How about data that do not seem linearly separable?**\n",
    "- We add a non-linear feature makes SVMs capable of linearly separating the data\n",
    "    - As you can see here, it seems that we cannot draw a linear line to split the data\n",
    "        - ![](svm1.png)\n",
    "    - However, we can add a new feature |x| to split the data\n",
    "    - The new points will be linearly separable\n",
    "        - ![](svm2.png)\n",
    "    - On the original plot, we can see this is how the data is separated\n",
    "        - ![](svm3.png)\n",
    "- It seems like we need to create new features, but there's a cool trick called the \"kernel trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Trick**\n",
    "- We can map non-separable data to a higher dimensionality to make it separable and then map back\n",
    "    - ![](svm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters Tuning with Scikit-Learn**\n",
    "- **Kernels**\n",
    "    - You can use common kernels and custom kernels\n",
    "        - Common kernels\n",
    "            - 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n",
    "- **C**\n",
    "    - Controls tradeoff between smooth decision boundary and classifying training points correctly\n",
    "    - The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example\n",
    "        - **Large C**\n",
    "            - The optimizer will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly \n",
    "        - **Small C** \n",
    "            - The optimizer will look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points\n",
    "                - For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable\n",
    "- **gamma**\n",
    "    - Defines how far the influence of a single example reaches\n",
    "        - **Low values**\n",
    "            - Far\n",
    "            - Only consider close points\n",
    "        - **High values**\n",
    "            - Low\n",
    "            - Even far-away points get considered\n",
    "            - You can end up with a wiggly decision boundary (over-fitting)\n",
    "    - Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVMs Suitability**\n",
    "- Does not work well with large datasets due to speed issues\n",
    "- Does not work well with a lot of noise\n",
    "    - Naive Bayes would be better\n",
    "- Works well for data that can be linearly classified"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

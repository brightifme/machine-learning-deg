{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Scaling and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**\n",
    "- Formula\n",
    "    - $$X' = \\frac {X - X_{min}}{X_{max} - X_{min}}$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms affected by feature rescaling**\n",
    "- Algorithms in which two dimensions affect the outcome will be affected by rescaling\n",
    "    - SVM with RBF kernel\n",
    "        - When you maximize the distance, you've 2 or more dimensions\n",
    "            - Think of the x and y axis with different dimensions and you need to calculate the distance\n",
    "    - K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling Manually in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.4166666666666667, 1.0]\n"
     ]
    }
   ],
   "source": [
    "### FYI, the most straightforward implementation might \n",
    "### throw a divide-by-zero error, if the min and max values are the same\n",
    "### but think about this for a second--that means that every\n",
    "### data point has the same value for that feature!  \n",
    "### why would you rescale it?  Or even use it at all?\n",
    "def featureScaling(arr):\n",
    "    \n",
    "    max_num = max(arr)\n",
    "    min_num = min(arr)\n",
    "    \n",
    "    lst = []\n",
    "\n",
    "    for num in arr:\n",
    "        X_prime = (num - min_num) / (max_num - min_num)\n",
    "        lst.append(X_prime)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "# tests of your feature scaler--line below is input data\n",
    "data = [115, 140, 175]\n",
    "print(featureScaling(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling in Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3 different training points for 1 feature\n",
    "weights = np.array([[115], [140], [175]]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.41666667],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rescale\n",
    "rescaled_weights = scaler.fit_transform(weights)\n",
    "rescaled_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**\n",
    "- Why do we want to select features?\n",
    "    - Knowledge discovery\n",
    "        - Interpretability\n",
    "        - Insight\n",
    "    - Curse of dimensionality\n",
    "    \n",
    "**Feature Selection: Algorithms**\n",
    "- How hard is the problem?\n",
    "    - Exponential\n",
    "        - $${n \\choose m}$$\n",
    "        - $$2^n$$\n",
    "            - n choose m\n",
    "            - Assuming our original number of features: n\n",
    "            - New number of features: m\n",
    "                - Where m <= n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Filtering vs Wrapping**\n",
    "- Disadvantages of **filtering**\n",
    "    - No feedback\n",
    "        - Learning algorithm cannot inform on the impact of the changes in features\n",
    "        - Criteria built in search with no reference to the learner\n",
    "        - Ignores learning problem\n",
    "    - You'll look at features in isolation\n",
    "- Advantages of **filtering**\n",
    "    - Fast\n",
    "- Disadvantages of **Wrapping**\n",
    "    - Slow\n",
    "- Advantages **Wrapping**\n",
    "    - Feedback\n",
    "        - Criteria built in the learner\n",
    "        - Takes into account model bias and learning\n",
    "![](feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering: Search Part**\n",
    "- We can use a Decision Tree algorithm for the search function then feed to the learner which does not do well with filtering features\n",
    "    - This is because DTs are good at filtering the best features\n",
    "    - If you want to know all the features, you can easily overfit\n",
    "- Other generic ways\n",
    "    - Information gain\n",
    "    - Entropy\n",
    "    - \"Useful\" features\n",
    "    - Independent, non-redundant\n",
    "    \n",
    "**Wrapping: Search Part (have to deal with the exponential problem)**\n",
    "- Hill climbing\n",
    "- Randomized optimization\n",
    "- Forward\n",
    "    - Start with m features\n",
    "    - You pass the features individually to the learning algorithms and get their scores\n",
    "    - You pick \n",
    "        - 2 features\n",
    "            - Choose highest score\n",
    "        - 3 features\n",
    "            - Choose highest score\n",
    "            - If lower than 2 features, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevance: Measures Effect on BOC**\n",
    "- X_i, feature, is strongly relevant if removing it degrades Bayes Optimal Classifier (BOC)\n",
    "    - BOC is the best you can do on average if you can find it\n",
    "- X_i, feature, is weakly relevant if \n",
    "    - Not strongly relevant\n",
    "    - Subset of features S such that adding x_i to S improves BOC\n",
    "- X_i is otherwise irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usefulness: Measures Effect on Particular Predictor**\n",
    "- Minimizing error given a model/learner\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
